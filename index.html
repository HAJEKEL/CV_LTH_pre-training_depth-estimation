<!DOCTYPE HTML>
<!--
	Massively by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>

<head>
    <title>Computer vision by deeplearning</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="assets/css/main.css" />
    <noscript>
        <link rel="stylesheet" href="assets/css/noscript.css" />
    </noscript>

</head>

<body class="is-preload">

    <!-- Wrapper -->
    <div id="wrapper" class="fade-in">

        <!-- Intro -->
        <div id="intro">
            <h1>Computer vision by deeplearning </h1>
            <h2>The Lottery Tickets Hypothesis for Supervised
                Pre-training in depth estimation models.</h2>
            <p>
                <a href="https://www.linkedin.com/in/henk-jekel-748054259/" target="_blank">
                    <img src="images/linkedin_icon.png" alt="LinkedIn Pictogram" width="35" height="35">
                </a>
                <a href="https://github.com/HAJEKEL/pt.darts" target="_blank">
                    <img src="images/github_icon.png" alt="Github Pictogram" width="35" height="35">
                </a>
                <a href="mailto:hendrikjekel@gmail.com" target="_blank">
                    <img src="images/gmail_icon.png" alt="Gmail Pictogram" width="35" height="35">
                </a>
            </p>
            <ul class="actions">
                <li>
                    <h4>Keep reading to learn more</h4>
                    <a href="#nav" class="button icon solid solo fa-arrow-down scrolly">Continue</a>
                </li>
            </ul>
        </div>


        <!-- Nav -->
        <nav id="nav">
            <ul class="links">
                <li class="active">
                    <a href="https://hajekel.github.io/">
                        <span style="font-size: 35px;">&larr;</span> Back to portfolio
                    </a>
                </li>
            </ul>




            <ul class="icons">
                <li><a href="https://www.linkedin.com/in/henk-jekel-748054259/"
                        class="icon brands alt fa-linkedin"><span class="label">LinkedIn</span></a>
                </li>
                <li><a href="https://www.kaggle.com/hajekel" class="icon brands fa-kaggle"><span
                            class="label">Kaggle</span></a></li>
                <li><a href="https://github.com/HAJEKEL" class="icon brands alt fa-github"><span
                            class="label">GitHub</span></a></li>
            </ul>
        </nav>

        <!-- Main -->
        <div id="main">

            <!-- Featured Post -->
            <article class="post featured">
                <header class="major">
                    <span class="date">April 28, 2023</span>
                    <h1>Computer vision by deeplearning </h1>
                </header>
                <h2>The Lottery Tickets Hypothesis for Supervised
                    Pre-training in depth estimation models. </h2>
                <!-- 
                <a class="image fit"><img src="images/EP50-reduce.png" alt="EP50-reduce" /></a>
                -->
                <h2>
                    Abstract
                </h2>
                <p>
                    Abstract:

                    In the field of computer vision, pre-trained models have gained renewed attention, particularly in
                    the realm of ImageNet supervised pre-training. Recent studies have highlighted the enduring
                    significance of the Lottery Tickets Hypothesis (LTH) in the context of classification, detection,
                    and segmentation tasks. Inspired by this, we set out to explore the potential of LTH in the
                    pre-training paradigm of depth estimation. Our aim is to investigate whether we can significantly
                    reduce the complexity of pre-trained models without compromising their downstream transferability in
                    the depth estimation task. Through extensive experimentation, we consistently identify highly sparse
                    matching subnetworks within pre-trained weights obtained from ImageNet classification. These
                    subnetworks demonstrate universal transferability to the depth estimation task, maintaining
                    performance comparable to that of the full pre-trained models. Our findings reinforce the relevance
                    of LTH in the pre-training paradigm of depth estimation, paving the way for more efficient and
                    effective depth estimation models.

                </p>
                <h2>
                    Introduction
                </h2>
                <p>
                    In the realm of computer vision, the resurgence of interest in pre-trained models, including
                    classical ImageNet supervised pre-training, has sparked enthusiasm. Recent studies suggest that the
                    core observations of the Lottery Tickets Hypothesis (LTH) remain relevant in the pre-training
                    paradigm of classification, detection, and segmentation tasks (Chen et al., Year). Additionally,
                    Jonathan Frankle and Michael Carbin's paper, "The Lottery Tickets Hypothesis for Supervised and
                    Self-supervised Pre-training in Computer Vision Models," contributes valuable insights to this
                    hypothesis.

                    Driven by curiosity, we pose the following question: Can we aggressively trim down the complexity of
                    pre-trained models without compromising their downstream transferability on the depth estimation
                    task?

                    In this engaging blog post, we explore the concept of supervised pre-trained models through the lens
                    of the Lottery Tickets Hypothesis (LTH) (Frankle & Carbin, 2020). LTH identifies highly sparse
                    matching subnetworks that can be trained almost from scratch and still achieve comparable
                    performance to the full models. Extending the scope of LTH, we investigate whether such matching
                    subnetworks exist in pre-trained computer vision models, specifically examining their transfer
                    performance in the context of depth estimation.

                    Our extensive experiments deliver an overall positive message: from the pre-trained weights obtained
                    through ImageNet classification, we consistently identify matching subnetworks at RESULT% to RESULT%
                    sparsity. These subnetworks demonstrate universal transferability to the depth estimation task,
                    exhibiting performance that remains on par with using the full pre-trained weights. Our findings
                    affirm the general relevance of the core observations of LTH within the pre-training paradigm of
                    depth estimation. The codes and pre-trained models used in our experiments can be accessed on
                    GitHub: https://github.com/HAJEKEL/CV_LTH_pre-training_depth-estimation.

                    Join us on this fascinating journey as we uncover the untapped potential of pre-trained models and
                    shed light on the intriguing relationship between the Lottery Tickets Hypothesis and depth
                    estimation in computer vision.

                    References:
                    Frankle, J., & Carbin, M. (2020). The Lottery Tickets Hypothesis for Supervised and Self-supervised
                    Pre-training in Computer Vision Models.

                    Chen, T., Frankle, J., Chang, S., Liu, S., Zhang, Y., Carbin, M., & Wang, Z. (Year). The Lottery
                    Tickets Hypothesis for Supervised and Self-supervised Pre-training in Computer Vision Models.
                </p>

                <h2>
                    Literature research
                </h2>

                <h3>Adabins</h3>
                <p>
                    Adabins backbone relies on efficientnet. To understand this backbone we guide you through the
                    evolutionary
                    path of the Fused-MBConv block, which is efficientnets main building block. The Fused-MBConv block
                    is a type of
                    convolutional neural network (CNN) block that has proven to be effective in improving the accuracy
                    and efficiency of image recognition models.

                    The Mobilenets Depthwise seperable convolution (DSConv) was one of the earliest architectural
                    innovations
                    that led to the development of the Fused-MBConv block. It involves a two-step convolution process
                    that separates the spatial and depth-wise convolutional operations, resulting in significant
                    reduction of the computational cost.

                    More details on DSConv in the downloadable pdf:


                <ul class="actions special">
                    <li><a href="MobileNets.pdf" class="button large">Download PDF</a></li>
                </ul>

                Visual impairment presents significant challenges for individuals worldwide, particularly when
                navigating unfamiliar environments, often resulting in potential injuries. Traditional solutions
                such as guide dogs and white canes have inherent limitations. While white canes provide feedback
                upon physical contact, they offer a finite line of perception in a three-dimensional space.
                Conversely, guide dogs map the entire three-dimensional world but possess a different perspective
                than humans, potentially leading to errors in collision estimations. This paper proposes a novel
                approach to overcome these limitations by utilizing a monocular camera in conjunction with a
                state-of-the-art monocular depth estimator to infer depth maps of the surroundings.

                Existing glasses designed for the visually impaired, such as those from Envision, primarily focus on
                object detection and provide audio messages for detected objects. However, this approach lacks
                contextual understanding and inundates users with overwhelming audio messages. This study aims to
                leverage inferred depth maps to provide haptic feedback, freeing visually impaired individuals from
                excessive audio messages and enabling them to navigate independently. To achieve this goal, it is
                crucial to reduce the size of state-of-the-art depth estimation models, as they are initially too
                large for implementation in existing glasses.

                To address this, the lottery ticket hypothesis will be applied to two state-of-the-art models: MiDaS
                v3.0 DPT-L and Adabins. These models were selected based on a comprehensive literature review,
                highlighting their potential as the fastest and most efficient options. MiDaS v3.0 DPT-L, in
                particular, was chosen due to its exceptional number of parameters compared to other versions with
                similar performance. It is anticipated that applying the lottery ticket hypothesis will
                significantly reduce the number of parameters and inference time. By implementing and integrating
                the reduced models into existing glasses, visually impaired individuals can navigate unfamiliar
                environments, avoid obstacles, and receive contextual haptic feedback, thereby enhancing their
                independence and safety.

                Motivated by the goal of empowering visually impaired individuals, this project focuses on
                developing a proof of concept implementation using a Raspberry Pi Model 3B. The pruned model
                analyzes a horizontal strip of average bins and evaluates three criteria based on the generated
                depth map to determine the optimal steering angle. The research aims to demonstrate the feasibility
                of incorporating compressed depth estimation models into embedded glasses, revolutionizing obstacle
                avoidance capabilities and enabling visually impaired individuals to navigate unfamiliar
                environments with improved context and independence.

                The inverted residuals and linear bottlenecks were introduced in the MobileNetV2 architecture, which
                is an extension of the Mobilenets. These architectural innovations helped to address the problem of
                poor accuracy in Mobilenets and further improved the computational efficiency of the model.

                The squeeze and excitation block is another architectural innovation that influenced the
                evolutionary path. The Squeeze-and-Excitation paper proposed a mechanism to selectively emphasize
                important channels while suppressing less important ones in convolutional layers to improve accuracy
                on image classification benchmarks.

                All of these architectural innovations eventually led to the development of the MBConv block, which
                incorporates the DSConv of Mobilenets, the inverted residuals and linear
                bottlenecks of MobileNetV2, and the squeeze and excitation block. The Fused-MBConv block combines
                two of the operations within the MBConv block, leading to faster computation.

                More details on mobilenetv2, MBConv and Fused-MBConv in the downloadable pdf:

                <ul class="actions special">
                    <li><a href="mobilenetv2_efficientnet.pdf" class="button large">Download PDF</a></li>
                </ul>
                </p>

                to be continued

                <h3> MiDaS</h3>

                to be continued

                <h3> Lottery ticket hypothesis</h3>

                The lottery ticket hypothesis, as described by Frankle and Carbin (2019), posits that a
                randomly-initialized, dense neural network contains a subnetwork that, when trained independently, can
                achieve the same test accuracy as the original network within a comparable number of training
                iterations. This implies that a significant portion of the network's parameters can be pruned, resulting
                in a computationally lighter model.

                <h4> Application</h4>
                In our specific case, the lottery ticket hypothesis holds promising potential as we aim to deploy these
                models on small single-board CPUs. To identify the lottery ticket parameters, we first train the
                complete model until it reaches an adequate level of training. Subsequently, we retrain the model using
                only the largest 10-20% of the weights, while maintaining the same initialization as the initial
                training process.
                <h2>
                    Methods
                </h2>
                <!-- 
                <p>
                    DARTS (Differentiable Architecture Search) is a method for automating the search for neural
                    network architectures. The goal is to find the best architecture for a given task without
                    requiring human expertise in designing neural networks.

                    In the search stage, the DARTS algorithm uses a differentiable relaxation of the architecture
                    search space to learn the best architecture. This involves learning the weights of the different
                    network operations in the architecture. The operations out of which DARTS could choose where
                    DSConv,
                    MBConv and Fused-MBConv.

                    The DARTS search was based on the Fashion-MNIST dataset. The Fashion-MNIST dataset is a popular
                    benchmark dataset used in machine learning and computer vision research. It consists of 70,000
                    grayscale images of 28x28 pixels each, divided into 10 classes, with 7,000 images per class. The
                    classes include T-shirts/tops, trousers, pullovers, dresses, coats, sandals, shirts, sneakers,
                    bags, and ankle boots. The dataset is a more challenging alternative to the classic MNIST
                    dataset, as it features more complex images with greater variability in the appearance of the
                    different classes.

                    It is worth noting that the distribution of the classes is roughly balanced, with each class
                    accounting for 10% of the dataset. This makes the dataset suitable for evaluating the
                    performance of machine learning algorithms in a multi-class classification setting.

                    In the context of DARTS (Differentiable Architecture Search), the terms "normal" and "reduce"
                    are used to refer to two distinct types of cells that are utilized in the process of
                    architecture search.

                    A "normal" cell is defined as a cell that maintains an unchanged spatial resolution between the
                    input and output, whereas a "reduce" cell is a cell that reduces the spatial resolution between
                    the input and output.

                    More details on DARTS in the downloadable pdf:

                </p>
                <ul class="actions special">
                    <li><a href="DARTS.pdf" class="button large">Download PDF</a></li>
                </ul>
                -->
                <h2>
                    Results
                </h2>
                <!-- 
                <p>
                    The experiments conducted involved training DARTS on the FashionMNIST dataset for 50 epochs,
                    utilizing the only available operations DSConv, MBConv, and Fused-mbconv. Two visualizations of
                    the training process were presented, which included a gif that displayed the progress of each
                    epoch for both the normal and reduce cells. Additionally, an image was provided that contained
                    two plots demonstrating the progress of the training: the first plot indicated the progression
                    of the loss throughout the training process, while the second plot showed the corresponding
                    accuracy progression. These visualizations and plots were indicative of the effectiveness and
                    efficiency of the training process, and demonstrated the potential of DARTS to be employed in
                    real-world tasks.





                    <a class="image fit"><img src="images/normal.gif" alt="normal" /></a>
                    <a class="image fit"><img src="images/reduce.gif" alt="reduce" /></a>
                    <a class="image fit"><img src="images/loss_accuracy.png" alt="loss_accuracy" /></a>

                    The training log can be found on github:
                <ul class="actions special">
                    <li><a href="https://github.com/HAJEKEL/pt.darts/blob/master/searchs/fashionmnist/fashionmnist.log"
                            class="button large">Visit training log</a></li>
                </ul>


                </p>
                -->
                <h2>
                    Conclusion
                </h2>

                --> <!-- 
                <p>
                    The high accuracy rates and low loss observed in both the training and validation indicate that
                    the training process was successful. The model has effectively learned the underlying patterns
                    in the data, as evidenced by the consistently low training loss. Furthermore, analysis of the
                    log of the last epoch suggests that the reduce operations generally outperform the normal
                    operations in terms of the softmax values for Alpha - normal and Alpha - reduce. This finding
                    suggests that the reduce operations are more important for achieving good performance on
                    FashionMNIST. Of note, the fusedmbconv operation has the highest alpha values among all
                    operations, underscoring its critical role in achieving strong results on the FashionMNIST
                    dataset. Taken together, these results demonstrate that the model has been trained effectively
                    and performs well on the given data.



                </p>
                -->
                <h2>
                    Code implementation
                </h2>
                <!-- 
                <p>
                    Code for this blog:
                <ul class="actions special">
                    <li><a href="https://github.com/HAJEKEL/EfficientNetV2_paper_reproduction/tree/website"
                            class="button large">WEBSITE</a></li>
                </ul>
                <p> Code to reproduce the training using google colab and google compute engine:
                </p>
                <ul class="actions special">
                    <li><a href="https://github.com/HAJEKEL/EfficientNetV2_paper_reproduction"
                            class="button large">TRAINING</a></li>
                </ul>
                <p> Code for the DARTS implementation with operations DSConv MBConv and Fused-MBConv:
                </p>
                <ul class="actions special">
                    <li><a href="https://github.com/HAJEKEL/pt.darts class="button large">DARTS</a></li>
                </ul>
                </p>
                -->
            </article>


        </div>

        <!-- Footer -->
        <footer id="footer">

            <section class="split contact">
                <section class="alt">
                    <h3>Address</h3>
                    <p>Delft, Netherlands<br />
                </section>

                <section>
                    <h3>Email</h3>
                    <p><a>hendrikjekel@gmail.com</a></p>
                </section>
                <section>
                    <h3>Social</h3>
                    <ul class="icons">
                        <li><a href="https://www.linkedin.com/in/henk-jekel-748054259/"
                                class="icon brands alt fa-linkedin"><span class="label">LinkedIn</span></a>
                        </li>
                        <li><a href="https://github.com/HAJEKEL" class="icon brands alt fa-github"><span
                                    class="label">GitHub</span></a></li>
                    </ul>
                </section>
            </section>
        </footer>

        <!-- Copyright -->
        <div id="copyright">
            <ul>
                <li>Henk Jekel &copy; 2022</li>
                <li>Design: <a href="https://html5up.net">HTML5 UP</a></li>
            </ul>
        </div>

    </div>

    <!-- Scripts -->
    <script src="assets/js/jquery.min.js"></script>
    <script src="assets/js/jquery.scrollex.min.js"></script>
    <script src="assets/js/jquery.scrolly.min.js"></script>
    <script src="assets/js/browser.min.js"></script>
    <script src="assets/js/breakpoints.min.js"></script>
    <script src="assets/js/util.js"></script>
    <script src="assets/js/main.js"></script>

</body>

</html>